{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738cfd17-f736-4e14-a351-fbdf2ab86314",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "CRlsCoQWopS2"
   },
   "source": [
    "## Spark ML Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887254d9-d388-449d-81df-a566991bfd77",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "Y0xzY8BUopS3"
   },
   "source": [
    "## Learning Outcomes\n",
    "Objectives:\n",
    "\n",
    "-  Use ML piplenes\n",
    "-  Improve a Random Forest model\n",
    "-  Perform Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0647d56c-b8cd-4cf0-bb6a-925216f4f991",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "uF923jNSopS4"
   },
   "source": [
    "Analysis 1: Predicting Farmer's Markets Locations\n",
    "\n",
    "From our preliminary analysis using the ML pipeline, we noted certain links but observed results that could be further optimized. Despite the limitations like potential invalid assumptions such as zip code shortening, there's room for refinement. One proposition is to consider neighboring zip codes or utilize distance measures to predict the likelihood of a farmer's market within a certain radius from affluent zip codes.\n",
    "\n",
    "The challenge is not strictly about enhancing the model's performance, but also about making iterative changes for potential improvements. The dataset in focus is the Farmers Markets dataset, which can be reviewed https://catalog.data.gov/dataset/farmers-markets-directory-and-geographic-data. For a baseline, refer to the model setup in this https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5915990090493625/2446126855165611/6085673883631125/latest.html.\n",
    "\n",
    "Analysis 2: Predicting Diamond Prices\n",
    "\n",
    "Leverage the power of the Apache Spark ML pipeline to construct a model predicting diamond prices based on the provided features. Detailed dataset information is available in this https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5915990090493625/4396972618536508/6085673883631125/latest.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6974a46-6385-4d0c-a8bd-3525eb2a85f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###For this assignment, I have shifted focus towards selecting features that could provide a more accurate representation of income and population in each zip code. To achieve this, data integration was carried out from two sources - the zcta_coordinates dataset for zip code coordinates, and the Census Bureau's API for population details corresponding to these zip codes. A haversine algorithm was employed to decrease granularity. Furthermore, a square root transformation was used to tackle data skewness, enhancing the model's performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7168ba5d-dbf7-4eb8-8dd7-35507b4b7ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame\n",
    "taxes2013 = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(\"dbfs:/databricks-datasets/data.gov/irs_zip_code_data/data-001/2013_soi_zipcode_agi.csv\")\n",
    "\n",
    "# Create a temporary view\n",
    "taxes2013.createOrReplaceTempView(\"taxes2013\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb7992f-605f-4e1b-829e-cdcb11d1a561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame\n",
    "markets = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(\"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001/market_data.csv\")\n",
    "\n",
    "# Create a temporary view\n",
    "markets.createOrReplaceTempView(\"markets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86fcde4-d2a6-45b6-893f-137e558bd0ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding coordinates for zip codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e397576-8820-4c13-8332-9263ef69a108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame\n",
    "zip_to_coords_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/zcta_coordinates-1.csv\")\n",
    "\n",
    "# Convert columns to appropriate types if necessary\n",
    "from pyspark.sql.functions import col\n",
    "zip_to_coords_df = zip_to_coords_df.withColumn(\"lon\", col(\"lon\").cast(\"double\"))\n",
    "zip_to_coords_df = zip_to_coords_df.withColumn(\"lat\", col(\"lat\").cast(\"double\"))\n",
    "\n",
    "# Create a temporary view for Spark SQL\n",
    "zip_to_coords_df.createOrReplaceTempView(\"zip_to_coords_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b8d040-4c33-4c65-b677-f54300144f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##API to populate tax zipcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24006af-b941-4a2b-891c-d7a5cd7c51ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+-------+\n|       Name|Population|State|Zipcode|\n+-----------+----------+-----+-------+\n|ZCTA5 25245|       600|   54|  25245|\n|ZCTA5 25268|       964|   54|  25268|\n|ZCTA5 25286|      1700|   54|  25286|\n|ZCTA5 25303|      6764|   54|  25303|\n|ZCTA5 25311|     10964|   54|  25311|\n|ZCTA5 25419|     11062|   54|  25419|\n|ZCTA5 25434|      2821|   54|  25434|\n|ZCTA5 25446|      1193|   54|  25446|\n|ZCTA5 25106|       598|   54|  25106|\n|ZCTA5 25501|      1198|   54|  25501|\n|ZCTA5 25507|       776|   54|  25507|\n|ZCTA5 25515|      2042|   54|  25515|\n|ZCTA5 25520|      1497|   54|  25520|\n|ZCTA5 25535|      2796|   54|  25535|\n|ZCTA5 25564|      1174|   54|  25564|\n|ZCTA5 25601|      5565|   54|  25601|\n|ZCTA5 25606|      1531|   54|  25606|\n|ZCTA5 25608|      1108|   54|  25608|\n|ZCTA5 25617|       706|   54|  25617|\n|ZCTA5 25621|      1626|   54|  25621|\n+-----------+----------+-----+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define the base URL for the API\n",
    "base_url = \"https://api.census.gov/data/2019/acs/acs5\"\n",
    "\n",
    "# Define the parameters for the API call\n",
    "params = {\n",
    "    \"get\": \"NAME,B01001_001E\",\n",
    "    \"for\": \"zip code tabulation area:*\"\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.get(base_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.text)\n",
    "    \n",
    "    # Convert the data to a DataFrame\n",
    "    rdd = sc.parallelize(data[1:])\n",
    "    df = rdd.map(lambda x: Row(Name=x[0], Population=int(x[1]), State=x[2], Zipcode=x[3])).toDF()\n",
    "\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"census_data\")\n",
    "    \n",
    "    df.show()\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1dad5a-a353-4e7d-99f5-595cc638e5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------+---------+\n|STATE|zipcode|agi_stub|       N1|\n+-----+-------+--------+---------+\n|   AL|  35004|       1|1530.0000|\n|   AL|  35004|       2|1330.0000|\n|   AL|  35004|       3| 910.0000|\n|   AL|  35004|       4| 610.0000|\n|   AL|  35004|       5| 510.0000|\n|   AL|  35004|       6|  40.0000|\n|   AL|  35005|       1|1390.0000|\n|   AL|  35005|       2|1030.0000|\n|   AL|  35005|       3| 470.0000|\n|   AL|  35005|       4| 230.0000|\n|   AL|  35005|       5| 180.0000|\n|   AL|  35005|       6|   0.0000|\n|   AL|  35006|       1| 440.0000|\n|   AL|  35006|       2| 330.0000|\n|   AL|  35006|       3| 190.0000|\n|   AL|  35006|       4| 130.0000|\n|   AL|  35006|       5| 140.0000|\n|   AL|  35006|       6|   0.0000|\n|   AL|  35007|       1|4300.0000|\n|   AL|  35007|       2|2680.0000|\n+-----+-------+--------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "taxes_selected = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        STATE, \n",
    "        zipcode, \n",
    "        agi_stub, \n",
    "        N1\n",
    "    FROM taxes2013\n",
    "    WHERE zipcode != 0 AND zipcode != 99999\n",
    "\"\"\")\n",
    "\n",
    "taxes_selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5cd869-f51b-4ab6-b696-3f626f4ed306",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###I have decided to shift the focus of my feature selection to fields that could provide a more accurate representation of the income for each zip code. Specifically, I have used N1 and agi_stub from the tax dataset, representing the number of tax returns and the respective income brackets. Using a weighted average approach, I estimated the adjusted gross income (AGI) for each zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d772cc-93b9-487a-af2c-d8c428852f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import when, col, sum\n",
    "\n",
    "# Define the median income for each agi_stub\n",
    "income_bracket = {1: 12500, 2: 37500, 3: 62500, 4: 87500, 5: 112500, 6: 212500}\n",
    "\n",
    "# Create a new column for the median income of each agi_stub\n",
    "for bracket, income in income_bracket.items():\n",
    "    taxes2013 = taxes2013.withColumn('agi_stub_{}'.format(bracket), \n",
    "                                     when(col('agi_stub') == bracket, col('N1') * income))\n",
    "\n",
    "# Group by zipcode and calculate the total count and total AGI\n",
    "taxes_aggregate = taxes2013.groupBy('zipcode').agg(\n",
    "    sum(col('N1')).alias('totalCount'),\n",
    "    sum(col('agi_stub_1')).alias('totalAGI_1'),\n",
    "    sum(col('agi_stub_2')).alias('totalAGI_2'),\n",
    "    sum(col('agi_stub_3')).alias('totalAGI_3'),\n",
    "    sum(col('agi_stub_4')).alias('totalAGI_4'),\n",
    "    sum(col('agi_stub_5')).alias('totalAGI_5'),\n",
    "    sum(col('agi_stub_6')).alias('totalAGI_6')\n",
    ")\n",
    "\n",
    "# Calculate the estimated AGI for each zipcode\n",
    "taxes_aggregate = taxes_aggregate.withColumn('estimated_AGI', \n",
    "    (col('totalAGI_1') + col('totalAGI_2') + col('totalAGI_3') + col('totalAGI_4') + col('totalAGI_5') + col('totalAGI_6')) / col('totalCount'))\n",
    "\n",
    "taxes_aggregate.createOrReplaceTempView(\"taxes_aggregate_view\")\n",
    "\n",
    "taxes_aggregate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16745497-55e5-4a0f-9a77-2d312808fec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+-----+------------------+------------------+\n|zipcode|estimated_AGI|Population|State|          Latitude|         Longitude|\n+-------+-------------+----------+-----+------------------+------------------+\n|  25817|      45192.0|       708|   54|         37.760812|-81.40611103544494|\n|  25501|      39881.0|      1198|   54|         38.159839|-81.96371335314505|\n|  25601|      43029.0|      5565|   54|        37.8546745|-81.99475838541792|\n|  25434|      39024.0|      2821|   54|        39.4573195|-78.41869813005647|\n|  25671|      38141.0|      1471|   54|37.896817999999996|-82.15554925432181|\n|  25311|      45595.0|     10964|   54|         38.362155|-81.54456745557027|\n|  25564|      44363.0|      1174|   54| 38.27144799999999|-81.90408735156251|\n|  25446|      54688.0|      1193|   54|          39.23663|  -77.944949777423|\n|  25515|      39286.0|      2042|   54|38.751594499999996|-82.14917737648028|\n|  25106|      34722.0|       598|   54|        38.7705875|-82.07345337355744|\n|  25245|      52244.0|       600|   54|        38.6973045|-81.72906310831095|\n|  25520|      39000.0|      1497|   54|         38.567425| -82.1674476159292|\n|  25621|      43414.0|      1626|   54|         37.621868|-81.88580678508771|\n|  25419|      50519.0|     11062|   54|         39.581675|-77.88504953124999|\n|  25632|      32143.0|       374|   54|         37.744203|-81.92145153338251|\n|  25535|      48246.0|      2796|   54| 38.31172099999999|-82.42846478809253|\n|  25608|      37500.0|      1108|   54|37.571121500000004| -81.8891345715017|\n|  25303|      48816.0|      6764|   54|        38.3593575|-81.68187436979284|\n|  25268|      37500.0|       964|   54|        38.7324205|-81.10125051804957|\n|  25617|      46371.0|       706|   54|        37.7323775|-81.78304674831621|\n+-------+-------------+----------+-----+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "# Adding Latitude and Longitude to the dataset\n",
    "\n",
    "joined_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        f.zipcode,\n",
    "        ROUND(f.estimated_AGI) as estimated_AGI,\n",
    "        c.Population,\n",
    "        c.State,\n",
    "        z.lat as Latitude,\n",
    "        z.lon as Longitude\n",
    "    FROM taxes_aggregate_view AS f\n",
    "    JOIN census_data AS c\n",
    "    ON f.zipcode = c.Zipcode\n",
    "    LEFT JOIN zip_to_coords_view AS z\n",
    "    ON f.zipcode = z.GEOID10\n",
    "\"\"\")\n",
    "\n",
    "joined_df.createOrReplaceTempView(\"joined_df\")\n",
    "\n",
    "joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09726c13-09b9-46bb-a915-23f3220f9e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipcode with null values:  0\nestimated_AGI with null values:  0\nPopulation with null values:  0\nState with null values:  0\nLatitude with null values:  0\nLongitude with null values:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "for column in joined_df.columns:\n",
    "    print(column, \"with null values: \", joined_df.where(col(column).isNull()).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ebfdfa-ff43-4272-ba4d-b5f897aab6df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####I will pick up the number of farmer markets for every zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6934e9e-7d5e-493b-8410-0b1b67052de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n|count| zip|\n+-----+----+\n|    5|4900|\n|    2|7240|\n|    8|4818|\n|    1|9852|\n|    2|5300|\n|    5|2122|\n|    2|9900|\n|    1|8592|\n|    1|1580|\n|    1|3175|\n+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as count, \n",
    "    INT(Zip / 10) as zip \n",
    "FROM markets \n",
    "GROUP BY INT(Zip / 10)\n",
    "\"\"\")\n",
    "\n",
    "result.createOrReplaceTempView(\"cleaned_markets\")\n",
    "\n",
    "result = spark.sql(\"SELECT * FROM cleaned_markets LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63544fd4-3316-4271-8803-fe7711472192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The final table consolidates relevant data from multiple sources into one comprehensive dataset. This dataset provides a detailed snapshot of each zip code area and features the following columns: zipcode;estimated_AGI;Population;Latitude;Longitude;count, and zip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f2928f-ee5e-45d6-9776-8db98ec62225",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+------------------+-------------------+-----+----+\n|zipcode|estimated_AGI|Population|          Latitude|          Longitude|count| zip|\n+-------+-------------+----------+------------------+-------------------+-----+----+\n|  35004|      49721.0|     12045|         33.615643| -86.48450973252687| null|null|\n|  35444|      44643.0|      3972|         33.330044| -87.29786008162166| null|null|\n|  35640|      47651.0|     25654|        34.4321505| -86.93239004931354| null|null|\n|  35670|      45462.0|      6594|34.462768499999996| -86.70405302288616| null|null|\n|  36067|      46077.0|     27601|        32.5138015| -86.59358225063184| null|null|\n|  36526|      57043.0|     34565|        30.6089265| -87.86754901099667| null|null|\n|  85022|      48409.0|     51920|        33.6250205|-112.05306184353742| null|null|\n|  72472|      34731.0|      9167|         35.575924| -90.53634864884637| null|null|\n|  90022|      28355.0|     67014|         34.029567|-118.15869319521369| null|null|\n|  91910|      45019.0|     74855|         32.636374|-117.05138249787463| null|null|\n|  92027|      44938.0|     56788|         33.133499|-116.98069255532114| null|null|\n|  93545|      46029.0|      1821| 36.57683349999999|-118.07034091790489| null|null|\n|  93924|      73283.0|      6514|         36.393057|-121.66104399576449| null|null|\n|  94102|      52774.0|     31392|37.779434499999994| -122.4203853802521| null|null|\n|  95134|      89978.0|     29349|37.423691500000004|-121.94573225584796| null|null|\n|  95519|      47806.0|     19763|40.960978999999995|-124.06082488832635| null|null|\n|  80305|      66895.0|     17126|        39.9722635|-105.24120865895063| null|null|\n|  80422|      55399.0|      4744|         39.854847|-105.56853570534096| null|null|\n|  80820|      49219.0|       882|        38.8169055|-105.54059992857142| null|null|\n|  06382|      47747.0|     12021|         41.463201| -72.10282876407274|    1|6382|\n+-------+-------------+----------+------------------+-------------------+-----+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Perform the join operation\n",
    "final_table = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        a.zipcode, \n",
    "        a.estimated_AGI, \n",
    "        a.Population, \n",
    "        a.Latitude,\n",
    "        a.Longitude,\n",
    "        b.count, \n",
    "        b.zip \n",
    "    FROM joined_df a \n",
    "    LEFT OUTER JOIN cleaned_markets b \n",
    "    ON(a.zipcode=b.zip)\n",
    "\"\"\")\n",
    "final_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a8beb0-6151-4d97-8721-5b13ebd9f765",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###I dropped zip becuase it is equivalent to the zipcode column and handle null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a52236d-c1d9-4569-bdae-eeee8bd9c0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+------------------+-------------------+-----+\n|zipcode|estimated_AGI|Population|          Latitude|          Longitude|count|\n+-------+-------------+----------+------------------+-------------------+-----+\n|  35004|      49721.0|     12045|         33.615643| -86.48450973252687|    0|\n|  35444|      44643.0|      3972|         33.330044| -87.29786008162166|    0|\n|  35640|      47651.0|     25654|        34.4321505| -86.93239004931354|    0|\n|  35670|      45462.0|      6594|34.462768499999996| -86.70405302288616|    0|\n|  36067|      46077.0|     27601|        32.5138015| -86.59358225063184|    0|\n|  36526|      57043.0|     34565|        30.6089265| -87.86754901099667|    0|\n|  85022|      48409.0|     51920|        33.6250205|-112.05306184353742|    0|\n|  72472|      34731.0|      9167|         35.575924| -90.53634864884637|    0|\n|  90022|      28355.0|     67014|         34.029567|-118.15869319521369|    0|\n|  91910|      45019.0|     74855|         32.636374|-117.05138249787463|    0|\n|  92027|      44938.0|     56788|         33.133499|-116.98069255532114|    0|\n|  93545|      46029.0|      1821| 36.57683349999999|-118.07034091790489|    0|\n|  93924|      73283.0|      6514|         36.393057|-121.66104399576449|    0|\n|  94102|      52774.0|     31392|37.779434499999994| -122.4203853802521|    0|\n|  95134|      89978.0|     29349|37.423691500000004|-121.94573225584796|    0|\n|  95519|      47806.0|     19763|40.960978999999995|-124.06082488832635|    0|\n|  80305|      66895.0|     17126|        39.9722635|-105.24120865895063|    0|\n|  80422|      55399.0|      4744|         39.854847|-105.56853570534096|    0|\n|  80820|      49219.0|       882|        38.8169055|-105.54059992857142|    0|\n|  06382|      47747.0|     12021|         41.463201| -72.10282876407274|    1|\n+-------+-------------+----------+------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "final_table = final_table.drop('zip')\n",
    "final_table = final_table.na.fill(0)\n",
    "final_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6fbcc0-66cd-435b-a90e-5acc81baa166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-------------------+-------------------+\n|summary|           zipcode|     estimated_AGI|        Population|         Latitude|          Longitude|              count|\n+-------+------------------+------------------+------------------+-----------------+-------------------+-------------------+\n|  count|             27687|             27687|             27687|            27687|              27687|              27687|\n|   mean| 48848.03991042727| 47430.19789070683|11640.184888214686|38.75023619422482| -90.26027897302043|0.06540975909271499|\n| stddev|27019.969998839104|13666.317327861723| 15360.77427909072|4.927206460175357| 14.048502786906592| 0.4598502109486731|\n|    min|             01001|           12500.0|                 0|        19.306708|-159.50799237610255|                  0|\n|    max|             99901|          141042.0|            128294|       66.1803405| -67.01129174440739|                 18|\n+-------+------------------+------------------+------------------+-----------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_table.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392db8f2-2b14-42d5-b1eb-b082e4dcb2a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Granularity and Grouping:The dataset's granularity, where each row corresponds to a single zip code, could potentially cause problems due to socioeconomic and geographic factors varying greatly even within small areas. To address this, I use the Haversine formula to group zip codes within a 10-km radius, thus creating a more representative local context. The resultant features for each group include: average neighborhood Adjusted Gross Income (AGI), average population, and count of neighboring markets. This approach should offer a more detailed and comprehensive basis for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f5f20a-86b1-4fd3-82e7-ac5f4bf5a2bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "# Define Haversine UDF\n",
    "def haversine_udf(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c1e6e0-36ac-4ada-b712-0ae2959c61d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the function as a UDF\n",
    "haversine_udf = F.udf(haversine_udf, DoubleType())\n",
    "\n",
    "# Add alias to differentiate the columns\n",
    "df1 = final_table.alias('df1')\n",
    "df2 = final_table.alias('df2')\n",
    "\n",
    "# Define a maximum distance (in kilometers)\n",
    "max_distance = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b44d04-dd79-4fa4-ad0c-41cdbd91f194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-join with distance condition and filter out self-join results\n",
    "neighbors = df1.crossJoin(df2).where(\n",
    "    (haversine_udf(F.col(\"df1.Longitude\"), F.col(\"df1.Latitude\"), F.col(\"df2.Longitude\"), F.col(\"df2.Latitude\")) <= max_distance) &\n",
    "    (F.col(\"df1.zipcode\") != F.col(\"df2.zipcode\")) # Exclude self-join results\n",
    ")\n",
    "\n",
    "# Filter pairs where at least one has a farmer's market\n",
    "neighbors_with_markets = neighbors.where((F.col(\"df1.count\") > 0) | (F.col(\"df2.count\") > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf7be76-9f20-48c5-b4aa-4a4c4cf1b461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-----------------------+--------------------------+\n|zipcode|avg_neighbor_agi|avg_neighbor_population|num_neighbors_with_markets|\n+-------+----------------+-----------------------+--------------------------+\n|  19121|         27640.0|                22874.0|                         3|\n|  19147|         41711.0|                16273.0|                         6|\n|  19102|         32640.0|                17916.0|                         4|\n|  19122|         27640.0|                22874.0|                         3|\n|  19136|         39040.0|                14868.0|                         2|\n|  19030|         52793.0|                 4123.0|                         1|\n|  19076|         58483.0|                 7642.0|                         1|\n|  19141|         33627.0|                18753.0|                         1|\n|  19140|         29281.0|                23343.0|                         2|\n|  19137|         41376.0|                21427.0|                         6|\n|  19145|         38854.0|                15138.0|                         4|\n|  19135|         34896.0|                21776.0|                         3|\n|  19106|         38357.0|                17999.0|                         5|\n|  19130|         27640.0|                22874.0|                         3|\n|  19133|         27640.0|                22874.0|                         3|\n|  55042|         66880.0|                27828.0|                         5|\n|  55125|         78629.0|                 9750.0|                         1|\n|  55003|         78629.0|                 9750.0|                         1|\n|  55082|         78629.0|                 9750.0|                         1|\n|  51333|         41731.0|                 1358.0|                         1|\n+-------+----------------+-----------------------+--------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregate functions to compute the mean of estimated_AGI, Population, and the count of neighbors with markets for each zipcode\n",
    "aggregations = [F.round(F.mean(\"df2.estimated_AGI\")).alias(\"avg_neighbor_agi\"),\n",
    "                F.round(F.mean(\"df2.Population\")).alias(\"avg_neighbor_population\"),\n",
    "                F.count(\"df2.zipcode\").alias(\"num_neighbors_with_markets\")]\n",
    "\n",
    "# Compute the average estimated_AGI, Population, and the count of neighbors with markets for each zipcode\n",
    "neighbors_with_features = neighbors_with_markets.groupby(\"df1.zipcode\").agg(*aggregations)\n",
    "\n",
    "neighbors_with_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3928d43-83dc-451b-aed5-3fa43cc3d517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipcode with null values:  0\navg_neighbor_agi with null values:  0\navg_neighbor_population with null values:  0\nnum_neighbors_with_markets with null values:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "for column in neighbors_with_features.columns:\n",
    "    print(column, \"with null values: \", neighbors_with_features.where(col(column).isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d7fb8d-f0af-4bde-836d-b373c13a40c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### I will examine the data for outliers, apply any necessary filters, then run the correlation analysis and random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aecc260-9217-4ea3-9c62-b8d47bf1139e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: this requires the matplotlib and seaborn libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Selecting the columns of interest\n",
    "data_df = neighbors_with_features.select([\"avg_neighbor_agi\", \"avg_neighbor_population\"]).toPandas()\n",
    "\n",
    "# Creating the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data_df)\n",
    "plt.xticks(rotation=90)  # Rotate the x labels for better readability if needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63677b4e-2da1-4871-8a4e-7c029d6a0ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound for avg_neighbor_agi: 20497.0\nUpper bound for avg_neighbor_agi: 91313.0\nLower bound for avg_neighbor_population: -20032.5\nUpper bound for avg_neighbor_population: 42683.5\nAdjusted lower bound for avg_neighbor_population: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Calculate bounds for avg_neighbor_agi\n",
    "quantiles_agi = neighbors_with_features.approxQuantile(\"avg_neighbor_agi\", [0.25, 0.75], 0.05)\n",
    "IQR_agi = quantiles_agi[1] - quantiles_agi[0]\n",
    "bounds_agi = [quantiles_agi[0] - 1.5 * IQR_agi, quantiles_agi[1] + 1.5 * IQR_agi]\n",
    "\n",
    "# Print bounds for avg_neighbor_agi\n",
    "print(f\"Lower bound for avg_neighbor_agi: {bounds_agi[0]}\")\n",
    "print(f\"Upper bound for avg_neighbor_agi: {bounds_agi[1]}\")\n",
    "\n",
    "# Calculate bounds for avg_neighbor_population\n",
    "quantiles_pop = neighbors_with_features.approxQuantile(\"avg_neighbor_population\", [0.25, 0.75], 0.05)\n",
    "IQR_pop = quantiles_pop[1] - quantiles_pop[0]\n",
    "bounds_pop = [quantiles_pop[0] - 1.5 * IQR_pop, quantiles_pop[1] + 1.5 * IQR_pop]\n",
    "\n",
    "# Print bounds for avg_neighbor_population\n",
    "print(f\"Lower bound for avg_neighbor_population: {bounds_pop[0]}\")\n",
    "print(f\"Upper bound for avg_neighbor_population: {bounds_pop[1]}\")\n",
    "\n",
    "# Adjust the lower bound for avg_neighbor_population\n",
    "bounds_pop[0] = max(0, bounds_pop[0])\n",
    "print(f\"Adjusted lower bound for avg_neighbor_population: {bounds_pop[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6874866-9145-4783-ab07-00cb8c8afc4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows where the avg_neighbor_agi is greater than 91313.0\n",
    "neighbors_with_features_filtered = neighbors_with_features.filter(neighbors_with_features.avg_neighbor_agi <= 91313.0)\n",
    "\n",
    "# Filter out rows where the avg_neighbor_population is greater than 42683.5\n",
    "neighbors_with_features_filtered = neighbors_with_features_filtered.filter(neighbors_with_features_filtered.avg_neighbor_population <= 42683.5)\n",
    "\n",
    "# Filter out rows where avg_neighbor_population is less than 0\n",
    "neighbors_with_features_filtered = neighbors_with_features_filtered.filter(neighbors_with_features_filtered.avg_neighbor_population >= 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9388100d-4d8b-403b-829c-af7a6862d88a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Parquet file\n",
    "neighbors_with_features_filtered.write.parquet(\"/FileStore/neighbors_with_features_filtered.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06db832e-23e3-4743-8fe0-4f72a786130f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Parquet file into a DataFrame\n",
    "neighbors_with_features_filtered = spark.read.parquet(\"/FileStore/neighbors_with_features_filtered.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d62031-d917-45f3-b802-948d8488df67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between avg_neighbor_agi and avg_neighbor_population: 0.3060124063084086\nCorrelation between avg_neighbor_agi and num_neighbors_with_markets: 0.19340331846648293\nCorrelation between avg_neighbor_population and num_neighbors_with_markets: 0.4476717926180829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = [col for col in neighbors_with_features_filtered.columns if col != 'zipcode']\n",
    "\n",
    "# Compute and print the correlation for each pair of columns\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):\n",
    "        print(f\"Correlation between {columns[i]} and {columns[j]}: {neighbors_with_features_filtered.stat.corr(columns[i], columns[j])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4206d4-593c-426e-a7a9-f02fad5a01c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The correlation values indicate that 'avg_neighbor_population' (0.448) has a stronger relationship with 'num_neighbors_with_markets' than 'avg_neighbor_agi' (0.193). These correlations are relatively modest, implying that further feature engineering might be beneficial. Nevertheless, this process highlights the importance of iterative feature selection and refinement in model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97fdb88-b1d1-41be-aebc-cc8264a0e150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Exclude non-feature columns\n",
    "featureCols = [\"avg_neighbor_agi\", \"avg_neighbor_population\"]\n",
    "\n",
    "# Use VectorAssembler to combine feature columns into a single vector column\n",
    "vectorAssembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "\n",
    "# Include label in the dataframe for modeling\n",
    "df_vector = vectorAssembler.transform(neighbors_with_features_filtered).select(\"features\", \"num_neighbors_with_markets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261fe5cd-2d88-4f62-8882-43c88f974b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 7.254474305541948\nMAE on test data = 4.007210198984731\nMSE on test data = 52.62739744976834\nR2 on test data = 0.2736195127336781\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Set the model\n",
    "rfModel = RandomForestRegressor(labelCol=\"num_neighbors_with_markets\", featuresCol=\"features\")\n",
    "\n",
    "# Set the pipeline\n",
    "pipeline = Pipeline().setStages([rfModel])\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "training, test = df_vector.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Specify the hyperparameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rfModel.numTrees, [10, 20]) \\\n",
    "    .addGrid(rfModel.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"num_neighbors_with_markets\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "# List of metrics we want to compute\n",
    "metrics = [\"rmse\", \"mae\", \"mse\", \"r2\"]\n",
    "\n",
    "# Evaluate the model on multiple metrics\n",
    "for metric in metrics:\n",
    "    evaluator = RegressionEvaluator(labelCol=\"num_neighbors_with_markets\", metricName=metric)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric.upper()} on test data = {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73965b9e-8ae7-4c83-af8b-df28f2b5c452",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Given the computational demands of running a more extensive hyperparameter grid, I had to streamline the parameters to ensure the model would run efficiently.\n",
    "###The model exhibits some predictive capability, but it's clear that there is substantial room for improvement. Notably, a significant amount of error and unexplained variance are present in the predictions.\n",
    "###As a pathway towards enhancing the model's performance, future work could consider:\n",
    "###Introducing additional features to the model\n",
    "###Undertaking more extensive feature engineering\n",
    "###Experimenting with different types of models"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2277304848212118,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "assignment4 Spark ML -FM second approach",
   "widgets": {}
  },
  "colab": {
   "name": "Assignment_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
